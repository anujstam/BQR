{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aySPCeSLwzh3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txzHIMQuBFxe"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "df_train = pd.read_csv('Datasets/IMDB/Train.csv', engine='python')\n",
    "df_test = pd.read_csv('Datasets/IMDB/Test.csv', engine='python')\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy',include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vbqccZar3uKp"
   },
   "outputs": [],
   "source": [
    "tv_datafields = [(\"text\", TEXT),(\"label\", LABEL)]\n",
    "train_data,  test_data = data.TabularDataset.splits(path='Datasets/IMDB/',\n",
    "                                        train=\"Train.csv\",\n",
    "                                        test=\"Test.csv\", format=\"csv\",\n",
    "                                        skip_header=True, fields=tv_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WEGxTE0d-VfG"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kd1PurTQ3VfO",
    "outputId": "c7f2c95c-6c75-4e3a-8e2c-41883e29306d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n",
      "100%|█████████▉| 399943/400000 [00:17<00:00, 22571.27it/s]"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iRvuY6kC3d7Z"
   },
   "outputs": [],
   "source": [
    "train_iterator,  test_iterator = data.BucketIterator.splits(\n",
    "    (train_data,  test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Lz_XRkpJ4_GS"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        result = self.fc(hidden)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6avjF2sB_uh3"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrsXi-lL_wUG",
    "outputId": "57d09e16-c977-43ed-c65a-a56464fcb4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n",
      "torch.Size([25002, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5590,  0.3448, -0.1751,  ..., -0.1506,  1.0337, -0.1511],\n",
       "        [-0.1991,  0.8435,  1.0862,  ...,  1.6896, -0.7945, -0.0032],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.1632, -0.5877,  0.6399,  ..., -0.0234,  0.0447,  0.8404],\n",
       "        [-0.2156,  1.0275,  0.0914,  ..., -0.3935,  0.8041, -0.6058],\n",
       "        [ 0.3393,  2.1684,  0.0894,  ..., -0.3708, -0.9544, -0.4810]])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCuvELGC_xx2",
    "outputId": "73966f07-e9a4-4256-e4fe-e78c43d482e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [-0.1632, -0.5877,  0.6399,  ..., -0.0234,  0.0447,  0.8404],\n",
      "        [-0.2156,  1.0275,  0.0914,  ..., -0.3935,  0.8041, -0.6058],\n",
      "        [ 0.3393,  2.1684,  0.0894,  ..., -0.3708, -0.9544, -0.4810]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dmBzdDHp_zH8"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w5O8FwLi_0QK"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "torch.manual_seed(12)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths.to('cpu')).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "g87RkHp2_1HZ"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths.to('cpu')).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "duuq9L6l_2Rp"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbjgbQrf_3W3",
    "outputId": "6a2ac495-dd40-4fd5-a754-82ce1f6fb026"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 399943/400000 [00:30<00:00, 22571.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 29s\n",
      "\tTrain Loss: 0.552 | Train Acc: 70.76%\n",
      "\tTest Loss: 0.390 | Test Acc: 81.39%\n",
      "Epoch: 02 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 0.319 | Train Acc: 86.81%\n",
      "\tTest Loss: 0.261 | Test Acc: 89.77%\n",
      "Epoch: 03 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 0.257 | Train Acc: 89.82%\n",
      "\tTest Loss: 0.235 | Test Acc: 90.86%\n",
      "Epoch: 04 | Epoch Time: 1m 34s\n",
      "\tTrain Loss: 0.220 | Train Acc: 91.44%\n",
      "\tTest Loss: 0.227 | Test Acc: 90.92%\n",
      "Epoch: 05 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 0.195 | Train Acc: 92.54%\n",
      "\tTest Loss: 0.247 | Test Acc: 90.66%\n",
      "Epoch: 06 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 0.173 | Train Acc: 93.53%\n",
      "\tTest Loss: 0.220 | Test Acc: 91.73%\n",
      "Epoch: 07 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 0.153 | Train Acc: 94.36%\n",
      "\tTest Loss: 0.211 | Test Acc: 92.11%\n",
      "Epoch: 08 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 0.139 | Train Acc: 95.00%\n",
      "\tTest Loss: 0.216 | Test Acc: 92.19%\n",
      "Epoch: 09 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 0.123 | Train Acc: 95.57%\n",
      "\tTest Loss: 0.219 | Test Acc: 92.13%\n",
      "Epoch: 10 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 0.113 | Train Acc: 96.02%\n",
      "\tTest Loss: 0.230 | Test Acc: 91.91%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yhBcWXNAE42z"
   },
   "outputs": [],
   "source": [
    "class Delta_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hidden_dim * 2, 9)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        quants = self.fc_1(hidden)\n",
    "        return quants\n",
    "\n",
    "    def penultimate(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        return hidden\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "czsPQX1tOKAY"
   },
   "outputs": [],
   "source": [
    "# Relevant functions\n",
    "\n",
    "all_qs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "all_qs = torch.Tensor(all_qs).to(device)\n",
    "mean_is = 0\n",
    "std_is = 1\n",
    "penalty = 1\n",
    "epsilon = 0.00\n",
    "\n",
    "def cumLaplaceDistribution(y_pred,mean,standard_deviation,all_qs):\n",
    "  aa = ((1-all_qs) * (y_pred - mean))/standard_deviation\n",
    "  aa.clamp_(max = 0)\n",
    "  lesser_term = all_qs * torch.exp(aa)\n",
    "  bb = (-1.0 * all_qs * (y_pred - mean))/standard_deviation\n",
    "  bb.clamp_(max = 0)\n",
    "  greater_term = 1 - ((1-all_qs) * torch.exp(bb))\n",
    "  dummy_ones = torch.ones_like(mean)\n",
    "  y_dummy_pred = torch.div(y_pred,dummy_ones)\n",
    "  y_dummy_pred[y_pred >= mean] = 1.0\n",
    "  y_dummy_pred[y_pred < mean] = 0.0\n",
    "  return ((1 - y_dummy_pred) * lesser_term )+  (y_dummy_pred * greater_term)\n",
    "\n",
    "def logLikelihoodLoss(y_true,y_pred,mean,standard_deviation,all_qs,reg_hook = False):\n",
    "  gf = cumLaplaceDistribution(0.0,mean = y_pred,standard_deviation = standard_deviation,all_qs = all_qs)\n",
    "  gf.clamp_(min = 1e-7,max = 1 - 1e-7)\n",
    "  if_one = y_true * torch.log(1 - gf)\n",
    "  if_zero = (1 - y_true) * torch.log(gf)\n",
    "  if(reg_hook):\n",
    "    return (if_one + if_zero)\n",
    "  else:\n",
    "    aa = - 1 * torch.mean(if_one + if_zero)\n",
    "    return aa \n",
    "\n",
    "def customLoss(y_true, y_pred, mean, standard_deviation, all_qs, penalty):\n",
    "    ind_losses = []\n",
    "    for i,j in enumerate(all_qs):\n",
    "      xyz = logLikelihoodLoss(y_true[:,0],y_pred[:,i] ,mean, standard_deviation, j)\n",
    "      ind_losses.append(xyz)\n",
    "    zero = torch.Tensor([0]).to(device)\n",
    "    dummy1 = y_pred[:,1:] - y_pred[:,:-1]\n",
    "    dummy2 = penalty * torch.mean(torch.max(zero,-1.0 * dummy1))\n",
    "    total_loss  = torch.mean(torch.stack(ind_losses)) +dummy2\n",
    "    return total_loss\n",
    "\n",
    "def customTestPred(y_pred,mean,standard_deviation,all_qs,batch_size = 1):\n",
    "  if(batch_size == 1):\n",
    "    acc = []\n",
    "    cdfs = []\n",
    "    eps = 1e-10\n",
    "    val = (y_pred - mean)/standard_deviation \n",
    "    for xx in range(batch_size):\n",
    "      if(y_pred < mean.item()):\n",
    "        lesser_term = all_qs * torch.exp((1 - all_qs) * val.item())\n",
    "        lesser_term  = 1 - lesser_term\n",
    "        cdfs.append(lesser_term.item())\n",
    "        if(lesser_term.item() >= 0.5):\n",
    "          acc.append([1])\n",
    "        else:\n",
    "          acc.append([0])\n",
    "\n",
    "      elif(y_pred >= mean.item()):\n",
    "        greater_term = 1 - ((1-all_qs) * torch.exp(-1 * all_qs * val.item()))\n",
    "        greater_term = 1 - greater_term\n",
    "        cdfs.append(greater_term.item())\n",
    "        if(greater_term.item() >= 0.5):\n",
    "          acc.append([1])\n",
    "        else:\n",
    "          acc.append([0])\n",
    "  elif(batch_size > 1):\n",
    "    acc = []\n",
    "    cdfs = []\n",
    "    eps = 1e-10\n",
    "    val = (y_pred - mean)/standard_deviation \n",
    "    for xx in range(batch_size):\n",
    "      if(y_pred < mean[xx]):\n",
    "        lesser_term = all_qs * torch.exp((1 - all_qs) * val[xx])\n",
    "        lesser_term  = 1 - lesser_term\n",
    "        cdfs.append(lesser_term.item())\n",
    "        if(lesser_term.item() >= 0.5):\n",
    "          acc.append([1])\n",
    "        else:\n",
    "          acc.append([0])\n",
    "\n",
    "      elif(y_pred >= mean[xx]):\n",
    "        greater_term = 1 - ((1-all_qs) * torch.exp(-1 * all_qs * val[xx]))\n",
    "        greater_term = 1 - greater_term\n",
    "        cdfs.append(greater_term.item())\n",
    "        if(greater_term.item() >= 0.5):\n",
    "          acc.append([1])\n",
    "        else:\n",
    "          acc.append([0])\n",
    "  return torch.Tensor(acc).to(device).reshape(-1,1),torch.Tensor(cdfs).to(device).reshape(-1,1)\n",
    "\n",
    "def customTestPred_new(op,batch_size = 1):\n",
    "  acc = []\n",
    "  cdfs = []\n",
    "  eps = 1e-10 \n",
    "  tau = torch.Tensor([0.5])\n",
    "  for xx in range(batch_size):\n",
    "    #ip_val = -1* op[xx]\n",
    "    ip_val = op[xx]\n",
    "    if(ip_val < 0):\n",
    "      predval = (1-tau)*torch.exp(tau*ip_val)\n",
    "      cdfs.append(predval.item())\n",
    "      if(predval.item() >= 0.5):\n",
    "        acc.append([1])\n",
    "      else:\n",
    "        acc.append([0])\n",
    "    elif(ip_val >= 0):\n",
    "      predval = 1-tau*torch.exp(-1*tau*op[xx])\n",
    "      cdfs.append(predval.item())\n",
    "      if(predval.item() >= 0.5):\n",
    "        acc.append([1])\n",
    "      else:\n",
    "        acc.append([0])\n",
    "  return torch.Tensor(acc).to(device).reshape(-1,1),torch.Tensor(cdfs).to(device).reshape(-1,1)\n",
    "\n",
    "def acc_Q(train_preds,train_labels):\n",
    "    train_preds = np.array(train_preds).reshape(-1,1)\n",
    "    train_labels = np.array(train_labels).reshape(-1,1)\n",
    "    cdfs_acc,_ = customTestPred(0,train_preds,standard_deviation = 1,all_qs = torch.Tensor([0.5]),batch_size = train_preds.shape[0])\n",
    "    count = 0\n",
    "    for i,j in zip(cdfs_acc,train_labels):\n",
    "      if(i.item() == j[0]):\n",
    "        count += 1\n",
    "    return count/train_labels.shape[0]\n",
    "\n",
    "def acc_tests(test_preds,test_labels):\n",
    "    test_preds = np.array(test_preds).reshape(-1,1)\n",
    "    test_labels = np.array(test_labels).reshape(-1,1)\n",
    "    cdfs_acc,_ = customTestPred(0,test_preds,standard_deviation = 1,all_qs = torch.Tensor([0.5]),batch_size = test_preds.shape[0])\n",
    "    count = 0\n",
    "    for i,j in zip(cdfs_acc,test_labels):\n",
    "      if(i.item() == j[0]):\n",
    "        count += 1\n",
    "    return count/test_labels.shape[0]\n",
    "\n",
    "def regular_acc(test_preds, test_labels):\n",
    "    test_preds = np.array(test_preds).reshape(-1,1)\n",
    "    test_labels = np.array(test_labels).reshape(-1,1)\n",
    "    count = 0\n",
    "    for i,j in zip(test_preds,test_labels):\n",
    "      if(i.item() < 0.5):\n",
    "        if j[0] == 0:\n",
    "          count +=1\n",
    "      else:\n",
    "        if j[0] == 1:\n",
    "          count +=1\n",
    "    return count/test_labels.shape[0]\n",
    "\n",
    "def cdf(mean, all_qs,epsilon = 0,standard_deviation = 1 ,y_pred= 0):\n",
    "  aa = ((1-all_qs) * (y_pred - mean))/standard_deviation\n",
    "  aa.clamp_(max = 0)\n",
    "  lesser_term = all_qs * torch.exp(aa)\n",
    "  lesser_term = 1 - lesser_term\n",
    "  bb = (-1.0 * all_qs * (y_pred - mean))/standard_deviation\n",
    "  bb.clamp_(max = 0)\n",
    "  greater_term = 1 - ((1-all_qs) * torch.exp(bb))\n",
    "  greater_term = 1 - greater_term\n",
    "  dummy_ones = torch.ones_like(mean)\n",
    "  y_dummy_pred = torch.div(y_pred,dummy_ones)\n",
    "  y_dummy_pred[y_pred >= mean] = 1.0\n",
    "  y_dummy_pred[y_pred < mean] = 0.0\n",
    "  return ((1 - y_dummy_pred) * lesser_term )+  (y_dummy_pred * greater_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Tp0ka25OXAB",
    "outputId": "d98530d3-e4a8-4d5c-c3d4-da0d86fd699c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,814,961 trainable parameters\n",
      "torch.Size([25002, 100])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [-0.1632, -0.5877,  0.6399,  ..., -0.0234,  0.0447,  0.8404],\n",
      "        [-0.2156,  1.0275,  0.0914,  ..., -0.3935,  0.8041, -0.6058],\n",
      "        [ 0.3393,  2.1684,  0.0894,  ..., -0.3708, -0.9544, -0.4810]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "\n",
    "d_model = Delta_RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "\n",
    "torch.manual_seed(12)\n",
    "target_acc = 0.90\n",
    "d_model = d_model.to(device)\n",
    "optimizer = torch.optim.Adam(d_model.parameters())\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(d_model):,} trainable parameters')\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)\n",
    "d_model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "d_model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "d_model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(d_model.embedding.weight.data)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_delta(model, optimizer, iterator, epochs):\n",
    "    train_preds_Q = []\n",
    "    train_labels = []    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        batch_size = labels.shape[0]\n",
    "        op_qs = model(text, text_lengths.to('cpu'))\n",
    "        lossQ = customLoss(labels.reshape(-1,1),op_qs,0,1,all_qs,penalty)\n",
    "        \n",
    "        lossQ.backward()\n",
    "  \n",
    "        optimizer.step()\n",
    "\n",
    "        for lag in op_qs[:,4].detach().reshape(-1,1):\n",
    "          train_preds_Q.append(lag.item())\n",
    "        for lag in labels.reshape(-1,1):\n",
    "          train_labels.append(lag.item())\n",
    "\n",
    "    acc_is_Q = acc_Q(train_preds_Q,train_labels)\n",
    "    return acc_is_Q\n",
    "\n",
    "\n",
    "def test_quantiles(model,loader,epochs):\n",
    "  model.eval()\n",
    "  test_preds_Q = []\n",
    "  test_preds_p = []\n",
    "  test_labels = []\n",
    "  with torch.no_grad():\n",
    "    for batch in loader:\n",
    "      labels = batch.label\n",
    "      text, text_lengths = batch.text\n",
    "      op_qs = model(text, text_lengths.to('cpu'))\n",
    "      for lag in op_qs[:,4].detach().reshape(-1,1):\n",
    "        test_preds_Q.append(lag.item())\n",
    "      for lag in labels.reshape(-1,1):\n",
    "        test_labels.append(lag.item())\n",
    "    acc_is_Q = acc_tests(test_preds_Q,test_labels)\n",
    "  return acc_is_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vi2xyDWoaPBl",
    "outputId": "3ada10e2-097a-4203-9ff0-9e50da717271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 51s\n",
      "\tTrain Acc: 58.55%\n",
      "\tTest Acc: 70.88%\n",
      "Epoch: 02 | Epoch Time: 1m 51s\n",
      "\tTrain Acc: 76.39%\n",
      "\tTest Acc: 71.72%\n",
      "Epoch: 03 | Epoch Time: 1m 51s\n",
      "\tTrain Acc: 85.84%\n",
      "\tTest Acc: 88.92%\n",
      "Epoch: 04 | Epoch Time: 1m 51s\n",
      "\tTrain Acc: 90.02%\n",
      "\tTest Acc: 89.26%\n",
      "Epoch: 05 | Epoch Time: 1m 51s\n",
      "\tTrain Acc: 91.55%\n",
      "\tTest Acc: 91.00%\n",
      "Epoch: 06 | Epoch Time: 1m 51s\n",
      "\tTrain Acc: 92.72%\n",
      "\tTest Acc: 90.74%\n",
      "Epoch: 07 | Epoch Time: 1m 51s\n",
      "\tTrain Acc: 93.69%\n",
      "\tTest Acc: 91.52%\n",
      "Epoch: 08 | Epoch Time: 1m 51s\n",
      "\tTrain Acc: 94.34%\n",
      "\tTest Acc: 91.86%\n",
      "Epoch: 09 | Epoch Time: 1m 51s\n",
      "\tTrain Acc: 95.08%\n",
      "\tTest Acc: 91.38%\n",
      "Epoch: 10 | Epoch Time: 1m 51s\n",
      "\tTrain Acc: 95.48%\n",
      "\tTest Acc: 90.14%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_acc = train_delta(d_model, optimizer, train_iterator,epoch)\n",
    "    test_acc = test_quantiles(d_model, test_iterator, epoch)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tTest Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIV226T2S6Cn",
    "outputId": "fe5afbee-b8cc-4c54-e4fb-7f2577dbec28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 715, 827, 1100, 1959, 40399]\n",
      "[0, 316, 258, 292, 334, 471]\n"
     ]
    }
   ],
   "source": [
    "total_per_delta = [0,0,0,0,0,0]\n",
    "wrong_per_delta = [0,0,0,0,0,0]\n",
    "d_model.eval()\n",
    "pflag = True\n",
    "\n",
    "for batch in train_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        batch_size = labels.shape[0]\n",
    "        op_qs = d_model(text, text_lengths.to('cpu'))\n",
    "        preds = [[] for i in range(9)]\n",
    "        lbls = []\n",
    "        for itemset in op_qs.detach():\n",
    "          for quant in range(9):\n",
    "            preds[quant].append(itemset[quant].item())\n",
    "        if not pflag:\n",
    "          for q in preds:\n",
    "            print(q)\n",
    "          print()\n",
    "        for lag in labels.reshape(-1,1):\n",
    "          lbls.append(lag.item())\n",
    "        for i in range(batch_size):\n",
    "          start = 4\n",
    "          delta = 0\n",
    "          while (delta<5 and not (preds[start-delta][i] < 0 < (preds[start+delta][i]))):\n",
    "            delta +=1\n",
    "          if not pflag:\n",
    "            print(delta)\n",
    "          total_per_delta[delta] +=1\n",
    "          if (preds[4][i]<=0 and lbls[i]==1) or  (preds[4][i]>0 and lbls[i]==0):\n",
    "            wrong_per_delta[delta] +=1\n",
    "        pflag = True\n",
    "\n",
    "print(total_per_delta)\n",
    "print(wrong_per_delta)\n",
    "\n",
    "for batch in test_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label\n",
    "        batch_size = labels.shape[0]\n",
    "        op_qs = d_model(text, text_lengths.to('cpu'))\n",
    "        preds = [[] for i in range(9)]\n",
    "        lbls = []\n",
    "        for itemset in op_qs.detach():\n",
    "          for quant in range(9):\n",
    "            preds[quant].append(itemset[quant].item())\n",
    "          for lag in labels.reshape(-1,1):\n",
    "            lbls.append(lag.item())\n",
    "        for i in range(batch_size):\n",
    "          start = 4\n",
    "          delta = 0\n",
    "          while (delta<5 and not (preds[start-delta][i] < 0 < (preds[start+delta][i]))):\n",
    "            delta +=1\n",
    "          total_per_delta[delta] +=1\n",
    "          if (preds[4][i]<=0 and lbls[i]==1) or  (preds[4][i]>0 and lbls[i]==0):\n",
    "            wrong_per_delta[delta] +=1\n",
    "\n",
    "# print(total_per_delta)\n",
    "# print(wrong_per_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3BHmDWpTFbQ",
    "outputId": "c6d14692-0ba8-4fef-aeae-82df3d1b402b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta      |0.10 | 0.20 | 0.30 | 0.40 | 0.50 | \n",
      "Misc. Rate |0.45 | 0.33 | 0.28 | 0.18 | 0.02 | \n",
      "Ret. Rate  |1.00 | 0.98 | 0.96 | 0.94 | 0.89 | "
     ]
    }
   ],
   "source": [
    "print(\"Delta      |\", end=\"\")\n",
    "for i in range(1,6):\n",
    "  print(\"{:.2f}\".format(i/10), end= ' | ')\n",
    "print()\n",
    "print(\"Misc. Rate |\", end=\"\")\n",
    "for i in range(1,6):\n",
    "  print(\"{:.2f}\".format(wrong_per_delta[i]/total_per_delta[i]), end= ' | ')\n",
    "print()\n",
    "print(\"Ret. Rate  |\", end=\"\")\n",
    "current_sum = 0\n",
    "total = sum(total_per_delta)\n",
    "for i in range(1,6):\n",
    "  retained = total - current_sum\n",
    "  print(\"{:.2f}\".format(retained/total), end= ' | ')\n",
    "  current_sum += total_per_delta[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eXjDmsBjKdq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB_Delta.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
